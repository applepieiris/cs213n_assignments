{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/applepieiris/cs213n_assignments/blob/main/assignment3_colab/assignment3/RNN_Captioning_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_EXM9sQP70EP",
        "outputId": "aa4707df-3e25-4c60-f256-89d1fe5c674c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
        "# assignment folder, e.g. 'cs231n/assignments/assignment3/'\n",
        "FOLDERNAME = 'cs213n_assignments/assignment3_colab/assignment3'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "# Now that we've mounted your Drive, this ensures that\n",
        "# the Python interpreter of the Colab VM can load\n",
        "# python files from within it.\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
        "\n",
        "# This downloads the COCO dataset to your Drive\n",
        "# if it doesn't already exist.\n",
        "%cd /content/drive/MyDrive/$FOLDERNAME/cs231n/datasets/\n",
        "!bash get_datasets.sh\n",
        "%cd /content/drive/MyDrive/$FOLDERNAME"
      ],
      "metadata": {
        "id": "Vqo7vyd18CS6",
        "outputId": "3ed8a13c-2464-4fd8-d77c-65ac5f638ed9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/cs213n_assignments/assignment3_colab/assignment3/cs231n/datasets\n",
            "/content/drive/MyDrive/cs213n_assignments/assignment3_colab/assignment3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup cell.\n",
        "import time, os, json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from cs231n.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n",
        "from cs231n.image_utils import image_from_url\n"
      ],
      "metadata": {
        "id": "GthzvUr98Nnn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load COCO data from disk into a dictionary.\n",
        "# We'll work with dimensionality-reduced features for the remainder of this assignment,\n",
        "# but you can also experiment with the original features on your own by changing the flag below.\n",
        "data = load_coco_data(pca_features=True) # 经过pca将原来4096维的features变成了512维\n",
        "\n",
        "# Print out all the keys and values from the data dictionary.\n",
        "for k, v in data.items():\n",
        "    if type(v) == np.ndarray:\n",
        "        print(k, type(v), v.shape, v.dtype)\n",
        "    else:\n",
        "        print(k, type(v), len(v))"
      ],
      "metadata": {
        "id": "yqJ-cGpB-vS9",
        "outputId": "1c32d6f4-d3c0-45c2-f7e4-3b19fd3cd679",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base dir  /content/drive/My Drive/cs213n_assignments/assignment3_colab/assignment3/cs231n/datasets/coco_captioning\n",
            "train_captions <class 'numpy.ndarray'> (400135, 17) int32\n",
            "train_image_idxs <class 'numpy.ndarray'> (400135,) int32\n",
            "val_captions <class 'numpy.ndarray'> (195954, 17) int32\n",
            "val_image_idxs <class 'numpy.ndarray'> (195954,) int32\n",
            "train_features <class 'numpy.ndarray'> (82783, 512) float32\n",
            "val_features <class 'numpy.ndarray'> (40504, 512) float32\n",
            "idx_to_word <class 'list'> 1004\n",
            "word_to_idx <class 'dict'> 1004\n",
            "train_urls <class 'numpy.ndarray'> (82783,) <U63\n",
            "val_urls <class 'numpy.ndarray'> (40504,) <U63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample a minibatch and show the images and captions.\n",
        "# If you get an error, the URL just no longer exists, so don't worry!\n",
        "# You can re-sample as many times as you want.\n",
        "batch_size = 3\n",
        "\n",
        "captions, features, urls = sample_coco_minibatch(data, batch_size=batch_size)\n",
        "for i, (caption, url) in enumerate(zip(captions, urls)):\n",
        "    plt.imshow(image_from_url(url))\n",
        "    plt.axis('off')\n",
        "    caption_str = decode_captions(caption, data['idx_to_word'])\n",
        "    plt.title(caption_str)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "oEQCLKGaAHqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_captions,train_features,train_urls = sample_coco_minibatch(data,batch_size=500,split='train')\n",
        "val_captions,val_features,val_urls = sample_coco_minibatch(data,batch_size=100,split='val')"
      ],
      "metadata": {
        "id": "TDJsH-FzNnrV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((train_features,train_captions))\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_features,val_captions))"
      ],
      "metadata": {
        "id": "OkoESbiUCSHn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds"
      ],
      "metadata": {
        "id": "D-PEGm3OLMgH",
        "outputId": "a0cbe2eb-97c3-449e-9bf3-1d76f00b1103",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset element_spec=(TensorSpec(shape=(512,), dtype=tf.float32, name=None), TensorSpec(shape=(17,), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1nXkM8XnLV91"
      },
      "execution_count": 29,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.10 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "20c115783511270f812f76a700d03d809984385649272eae625a8ef0834ef776"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}